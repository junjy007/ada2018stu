{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.optim import Adam\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DDIR = os.path.expanduser(\"~/data/common\")\n",
    "if not os.path.exists(DDIR):\n",
    "    os.makedirs(DDIR)\n",
    "mnist_transform = transforms.Compose([transforms.ToTensor()])\n",
    "mnist_trainset = torchvision.datasets.MNIST(\n",
    "    root=DDIR, train=True, download=True,\n",
    "    transform=mnist_transform)\n",
    "mnist_trainloader = torch.utils.data.DataLoader(\n",
    "    mnist_trainset, batch_size=32,\n",
    "    shuffle=True, num_workers=2\n",
    ")\n",
    "mnist_testset = torchvision.datasets.MNIST(\n",
    "    root=DDIR, train=False, download=True,\n",
    "    transform=mnist_transform)\n",
    "mnist_testloader = torch.utils.data.DataLoader(\n",
    "    mnist_testset, batch_size=32\n",
    ")\n",
    "\n",
    "#### Helper functions ####\n",
    "\n",
    "def show(img):\n",
    "    npimg = img.detach().numpy()\n",
    "    npimg -= npimg.min()\n",
    "    npimg /= npimg.max()\n",
    "    if npimg.shape[0] in [3,4]:\n",
    "        plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "    else:\n",
    "        plt.imshow(npimg.squeeze(), interpolation='nearest', cmap='gray')\n",
    "        \n",
    "def draw_balls(C, sample=None):\n",
    "    plt.figure(1, figsize=(20,1))\n",
    "    s = 1024\n",
    "    for i, c_ in enumerate(C):\n",
    "        plt.scatter(i+1, 1, s=s, c=c_)\n",
    "    if sample is not None:\n",
    "        plt.scatter(sample+1, 1, s=s*1.5, c='none', edgecolors=['k',])\n",
    "    plt.xlim([0, len(C)+1])\n",
    "    plt.ylim([0.9, 1.1])\n",
    "    #axis('equal')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "def play_game(urn, seed=None):\n",
    "    if seed is not None:\n",
    "        rng = np.random.RandomState(seed)\n",
    "    else:\n",
    "        rng = np.random.RandomState()\n",
    "        \n",
    "    games = 0\n",
    "    guesses = []\n",
    "    while True:\n",
    "        sample = rng.randint(len(urn))\n",
    "        draw_balls(urn, sample)\n",
    "        g = int(input(\"How many guesses: \"))\n",
    "        guesses.append(g)\n",
    "        games += 1\n",
    "        \n",
    "        \n",
    "        more = input(\"Play more? [y]|n\")\n",
    "        if more=='n' or more=='N':\n",
    "            break\n",
    "        \n",
    "    avg_guesses = np.mean(guesses)    \n",
    "    print (\"Finished: {}, average questions\".format(guesses, avg_guesses))\n",
    "    return avg_guesses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Where are we -- in the learning framework?\n",
    "\n",
    "\n",
    "<img src=\"ref/learning.png\" alt=\"LearningFramework\" height=\"400\" width=\"500\">\n",
    "\n",
    "The major players:\n",
    "- data\n",
    "- models (hypotheses)\n",
    "- algorithm\n",
    "- <span style=\"color:red;font-weight:bold\"> selection criterion </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Quantifying Information\n",
    "\n",
    "A systematic way of \"how much my model has told me about the answer?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Good criteria\n",
    "\n",
    "It is related to the form of the target of prediction and the model output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Relevant to the ultimate goal. \n",
    "- Differentiable "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Entropy\n",
    "\n",
    "The goal is to compare two systems. Let us first consider the information in ONE \"system\" -- probabilistic distribution. And by the way, derive an effeicient shceme of _asking questions_, i.e. the _motivation_ behind most decision tree algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To take the notion of _information_ specifically, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- the minimum average code length to communicate an element drawn from the system.\n",
    "    - the codec-scheme must be determined BEFOREhand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- how many ways you can arrange the stuff in a system -- the number of status of a system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cost of A Guessing Game\n",
    "Let us start from a guessing GAME to understand the concept of \"measuring the information\": \n",
    "```\n",
    "Consider Bob has an urn containing colour balls.  Alice know the contents in the urn.  Now Bob draw one ball from the urn, Alice wants to know the colour of the ball. Bob can answer \"yes/no\" questions, but charge one dollar per question. \n",
    "```\n",
    "\n",
    "The goal is to design an asking scheme for Alice, so avargely she pays the lest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### GAME-1\n",
    "Try water ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "urn = ['r', 'r', 'r', 'r', 'b', 'b', 'b', 'b']\n",
    "draw_balls(urn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "play_game(urn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# GAME-2\n",
    "More elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Game 2\n",
    "urn = ['r', 'r', 'y', 'y', 'b', 'b', 'g', 'g']\n",
    "draw_balls(urn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "play_game(urn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### GAME-3\n",
    "Maybe we can allocate codes smartly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Game 3\n",
    "urn = ['r', 'r', 'r', 'r', 'b', 'b', 'g', 'y']\n",
    "draw_balls(urn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "play_game(urn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# GAME-4\n",
    "It all about UNcertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "urn = ['b', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n",
    "draw_balls(urn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "play_game(urn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scheme\n",
    "Design principles:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- earlier questions should identify frequent elements; rare elements are concerned later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Code length: $\\log_2 \\frac{1}{p_i}$ for event with probability $p_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Entropy\n",
    "\n",
    "Definition w.r.t. minimum coding length:\n",
    " $$ \\sum_i p_i \\log_2 \\frac{1}{p_i} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Sum over all elements\n",
    "- why $p_i$\n",
    "- motivation of $\\log (1/p)$\n",
    "    - when $p$ is large\n",
    "    - when $p$ is small\n",
    "- why base-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Couting System States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "urns = [['b', 'b', 'b', 'b', 'b', 'b', 'b', 'r'],\n",
    "        ['b', 'b', 'b', 'b', 'b', 'b', 'r', 'b'],\n",
    "        ['b', 'b', 'b', 'b', 'b', 'r', 'b', 'b'],\n",
    "        ['b', 'b', 'b', 'b', 'r', 'b', 'b', 'b'],\n",
    "        ['b', 'b', 'b', 'r', 'b', 'b', 'b', 'b'],\n",
    "        ['b', 'b', 'r', 'b', 'b', 'b', 'b', 'b'],\n",
    "        ['b', 'r', 'b', 'b', 'b', 'b', 'b', 'b'],\n",
    "        ['r', 'b', 'b', 'b', 'b', 'b', 'b', 'b']]\n",
    "\n",
    "for u_ in urns:\n",
    "    draw_balls(u_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are $\\left(\\begin{array}{c}\n",
    "8\\\\\n",
    "1\n",
    "\\end{array}\\right)=8$ _different_ ways of arranging the 8 balls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Q__: How many different ways to arrange the balls if there are 2 red balls and 6 blue ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are $$\\left(\\begin{array}{c}\n",
    "8\\\\\n",
    "2\n",
    "\\end{array}\\right)=\\frac{8!}{2!\\cdot 6!}$$ _different_ ways of arranging the 8 balls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(8*7*6*5*4*3*2*1/((2*1)*(6*5*4*3*2*1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Why more than the 1-red-7-blue case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Counting possible states of $N$-element System\n",
    "\n",
    "Ways to organise $N$ elements, and there are $p_1 \\cdot N$ elements of the first kinds, $p_2 \\cdot N$ elements of the second kinds, ... (totally $k$ kinds)\n",
    "\n",
    "$$\n",
    " \\frac{N!}{(p_{1}N)!(p_{2}N)!\\dots(p_{k}N)!}\\\\\n",
    "\\simeq \\frac{N^{N}}{(p_{1}N)^{p_{1}N}(p_{2}N)^{p_{2}N}\\dots(p_{k}N)^{p_{k}N}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The approximation is according to [Stirling's approximation][1], where \n",
    "\n",
    "> $\\log (N!) \\simeq N \\log N - N + O(N) $\n",
    "\n",
    "[1]:https://en.wikipedia.org/wiki/Stirling%27s_approximation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The system entropy is \n",
    "$$ S = - \\sum_i^k p_i \\log_2 {p_i} $$\n",
    "then the number of status is $$2^{N\\cdot S}$$\n",
    "\n",
    "That is, one sample from the system carries the information: \n",
    "> Hey, this is the particular one in $2^{NS}$.\n",
    "\n",
    "Removing the factor of $N$ (which we always let grow to infinity in this kind of analysis), the entropy $S$ is the average bits of information each sample carries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluating Model\n",
    "\n",
    "Consider that the ground-truth answer of a classification problem is 2 in $[0, 1, 2]$, and your model's output is $(0.1, 0.1, 0.8)$ If someone make a coding scheme according to your model's prediction about the labels:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "i.e. transmitting \n",
    "- event $Y=0$ with code length $- \\log_2 0.1$, \n",
    "- and $Y=1$ with code length $- \\log_2 0.1$, \n",
    "- and $Y=2$ with code length $- \\log_2 0.8$. \n",
    "\n",
    "If the $Y$-variable associated with the data sample $X$ is truly $[0.1, 0.1, 0.8]$ as predicted, the coding scheme is optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Q__: What is the optimal expected coding length?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "However, the actual \"probability\" is \n",
    "- $Pr$(The sample belongs to Class-0) $=0$\n",
    "- $Pr$(The sample belongs to Class-1) $=0$\n",
    "- $Pr$(The sample belongs to Class-2) $=1.0$\n",
    "The actual coding cost would be $$0 + 0  - \\log_2 0.8 \\simeq 0.322$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This is of course shorter than the expected code length of given the prediction by the model. However, the \"true\" optimal coding should be $0+0+log_2 1.0 = 0$, i.e. if the prediction is 100% correct and confident, there would not be any need of extra coding at all. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So we introduce the concept of cross-entropy. The intution is that \n",
    "> I have my version of the probabiliy of events and made coding scheme accordingly. However, the actual distribution is otherwise. \n",
    "\n",
    "Note, the difference between the optimal coding scheme is called KL-Divergence. [Here](http://octavia.zoology.washington.edu/teaching/429/lecturenotes/lecture3.pdf) is a comprehensive introduction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Tress -- A Revisit\n",
    "\n",
    "Below is a pseudo-code implementation of ID3 algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```\n",
    "# 31005 Students can use for A2\n",
    "\n",
    "def id3(examples, target, attrs):\n",
    "  create root node for tree\n",
    "  \n",
    "  if examples all +ve, \n",
    "    return root with label=+ \n",
    "  \n",
    "  if examples all -ve, \n",
    "    return root with label=- \n",
    "  \n",
    "  if attrs is empty\n",
    "    return root w/ label=most common value of target in examples \n",
    "  else \n",
    "    A ← attribute from attrs that best splits examples \n",
    "\n",
    "  root ← A\n",
    "  for each possible value vi in A\n",
    "    add a new branch below root corresp. to the test A = vi \n",
    "    examples_vi ← the subset of examples with A = vi\n",
    "  \n",
    "  if examples_vi is empty\n",
    "    add a leaf node below branch w/ label = most common value of target from examples\n",
    "  else \n",
    "    below the branch add the subtree given by id3(examples_vi, target, attrs - { A })\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Decision trees \"would like to\" lead you (using the branch at each node) to a sub-population of the data that is purer than the parent node-population.\n",
    "\n",
    "The basic ideas behind decision tree building and of minimising cross-entropy are similar:\n",
    "\n",
    "__Let what a model says about an $X$ tells most about the corresponding $Y$__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Q/A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__LAB_Q__\n",
    "Compute the entropy of \n",
    "- 0.125 * np.log2(0.125) - 0.675 * np.log2(0.675)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__LAB_Q__\n",
    "How about the number of combinations of 6 blue balls, 1 red ball and 1 yellow ball? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__LAB_Q__:\n",
    "Coding: given $N$ and $p<N$, compute the LOG-value of\n",
    "$$\n",
    "  \\frac{N!}{(pN)!((1-p)N)!}\\\\\n",
    "\\simeq  \\frac{N^{N}}{(pN)^{pN}((1-)N)^{(1-p)N}}\n",
    "$$\n",
    "\n",
    "Try different ($N$, $p$) combinations, and show they are relatively close when $N$ is large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "f = lambda k: (torch.arange(k)+1).log_().sum().item()\n",
    "g = lambda k: math.log(k)*k\n",
    "h = lambda n, m, f_: f_(n) - f_(m) - f_(n-m)\n",
    "print(h(10000000, 60000, f), h(10000000, 60000, g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__LAB_Q__:\n",
    "\n",
    "Consider a classification task: e.g. our old friend, the hand-written digit classifier. Let's check the model and its output on our images.\n",
    "\n",
    "Define a hand-written digit classifier, with 10 outputs corresponding to the digits. Try\n",
    "- Negative log likelihood loss\n",
    "- MSE loss\n",
    "- L1 loss\n",
    "Discuss your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class HandwrittenDigitNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HandwrittenDigitNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, \n",
    "                               out_channels=64, \n",
    "                               kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, \n",
    "                               out_channels=128, \n",
    "                               kernel_size=3)\n",
    "        self.linear = nn.Linear(3200, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: a batch of images\n",
    "        \"\"\"\n",
    "        h = self.conv1(x)\n",
    "        h = F.leaky_relu(h, 0.2, inplace=True)\n",
    "        h = F.max_pool2d(h, 2)\n",
    "        h = self.conv2(h)\n",
    "        h = F.leaky_relu(h, 0.2, inplace=True)\n",
    "        h = F.max_pool2d(h, 2)\n",
    "        h = self.linear(h.view(h.shape[0], 3200))\n",
    "        h = F.log_softmax(h, dim=1)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = HandwrittenDigitNet()\n",
    "for X, y in mnist_trainloader:\n",
    "    h = mnist_net(X)\n",
    "    break\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Q__: What is the output for an input sample in this example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Q__: what is the desirable output of a particular sample?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
