{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "## Take-home points -- Lecture\n",
    "- NN is constructed from MLP\n",
    "- Linear OPs define individual layers (even seems not so, OP for operation/operator)\n",
    "    - Matrix-vector production is the workhorse\n",
    "- Non-linear OPs support the network body structure\n",
    "- Training is to choose a set of parameters -- weights in each layer -- by changing from a starting point, slowly.\n",
    "- Algorithm to determine the direction of change\n",
    "\n",
    "Keywords (\"smart words\" I can talk to people and don't afraid they asking \"what do you mean by ...?\" after this class!):\n",
    "`deep convolutional neural network`, `back-propagation`, `regularisation`, `learning rate`, `stochastic gradient descent`\n",
    "\n",
    "\n",
    "## Take-home points -- Lab\n",
    "- a \"tensor\" object (personally, this is a misnormer)\n",
    "- end-to-end neural network learning (when the data allows)\n",
    "    1. build net\n",
    "    2. select loss\n",
    "    3. optimiser\n",
    "    \n",
    "Collectables: `GPU-deep learning`, `pytorch` (`tensorflow` if you choose to learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 MLP\n",
    "\n",
    "Recall our old friend linear model:\n",
    "\n",
    "- ONE linear model accesses ALL _observable_ attributes of the data, and produces ONE answer.\n",
    "    - Form the final analytics using this answer -- linear model completed\n",
    "    \n",
    "__Let's push for an MLP__\n",
    "- Employ MULTIPLE, say, $n$, linear models, and \n",
    "- Treat the $n$ answers as new attributes\n",
    "- Build another layer of linear models on top of the $n$ answers\n",
    "\n",
    "![NN as MLP][fig:mlp]\n",
    "\n",
    "[fig:mlp]: ref/illu-1.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q1__\n",
    "1. If a data sample consists of 10 attributes, how many parameters in a linear model? What is the Hypotheses space $\\mathcal{H}$? (In this class, we always ignore the bias, which we can treatas a weight on a constant-1 attribute)\n",
    "2. How many parameters we need to specify the model for 3 data samples?\n",
    "3. How many parameters we need to specify 5 such models?\n",
    "4. If I build a second layer of linear model, taking as input the outputs of the first layer, and produce the final answer. How many parameters in the entire model?\n",
    "5. (opt) Consider a practical model, where the inputs are images of $64 \\times 64$ RGB pixels, the first layer has 4,096 units (linear models), the second, third and forth layers have 1,024 units each, finally, it outputs 10 predictions, say the plausibility that input image belongs to 10 different classes. Specifically $[X \\in \\mathbb{R}^{64\\times 64}] \\mapsto  [H^1 \\in \\mathbb{R}^{4096}] \\mapsto  [H^2 \\in \\mathbb{R}^{1024}] \\mapsto  [H^3 \\in \\mathbb{R}^{1024}] \\mapsto  [H^4 \\in \\mathbb{R}^{1024}] \\mapsto  [Y \\in \\mathbb{R}^{10}]$. Figure out the hypotheses space of the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A1__\n",
    "\n",
    "- $\\mathcal{H}$ is $\\mathbb{R}^{10}$\n",
    "- the same, 10\n",
    "- 50\n",
    "- 50 + 5=55, the extra 5 weights are for the second layer model\n",
    "- (opt) see lab solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Lab__\n",
    "\n",
    "1. Build a linear model of 10 attributes using `pytorch`, and count the parameters; then let the linear model output 5 results, and count the parameters\n",
    "\n",
    "    - Hint-1: In torch module `torch.nn`, there is a [Linear] class.\n",
    "    - Hint-2: When providing the class the information about the number of output-\"feature\"s, consider that the linear model will give THE final answer.\n",
    "    - Hint-3: Each \"Neural network module\" class in `torch.nn` has a [access-to-parameters] method providing reference to the internal parameters. Note linear models _optionally_ contain bias.\n",
    "    - Hint-4: check the example below, note the use of `np.prod`.\n",
    "    \n",
    "2. Program the model in the last question of __Q1__\n",
    "\n",
    "[Linear]: https://pytorch.org/docs/stable/nn.html#linear-layers\n",
    "[access-to-parameters]: https://pytorch.org/docs/stable/nn.html#torch.nn.Module.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np # for a convenient cumulative product\n",
    "linear_model = nn.Linear(\n",
    "    in_features=10, out_features=1, bias=False)\n",
    "for i_, param in enumerate(linear_model.parameters()):\n",
    "    s = param.size()\n",
    "    print(\"Para {}: type {}, size {}, #.elements {}\".format(\n",
    "        i_, type(param.data), s, np.prod(s)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ANSWER TO Q1_Lab.2\n",
    "# X->H1\n",
    "layer1 = nn.Linear(in_features=64*64, out_features=4096, bias=False)\n",
    "# H1->H2\n",
    "layer2 = nn.Linear(in_features=4096, out_features=1024, bias=False)\n",
    "# H2->H3\n",
    "layer3 = nn.Linear(in_features=1024, out_features=1024, bias=False)\n",
    "# H3->H4\n",
    "layer4 = nn.Linear(in_features=1024, out_features=1024, bias=False)\n",
    "# H4->Y\n",
    "layer5 = nn.Linear(in_features=1024, out_features=10, bias=False)\n",
    "\n",
    "import numpy as np \n",
    "i = 0\n",
    "total_parasize = 0\n",
    "for model in (layer1, layer2, layer3, layer4, layer5):\n",
    "    for param in model.parameters():\n",
    "        s = param.size()\n",
    "        print(\"{}:{}\".format(i, s))\n",
    "        total_parasize += np.prod(s)\n",
    "        i += 1\n",
    "print(\"Total parameter number is {}\".format(total_parasize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Unified formulation of the computation\n",
    "\n",
    "Consider a data sample of 3 attributes, with a linear model with 3 weights: $(x_1, x_2, x_3)$ and $w_1, w_2, w_3$. The computation is $$\n",
    "x_1 w_1 + x_2 w_2 + x_3 w_3\n",
    "$$\n",
    "\n",
    "Let us write this product-sum in a format which allows extensive generalisation:\n",
    "\n",
    "$[\\begin{array}{ccc}\n",
    "x_{1} & x_{2} & x_{3}\\end{array}]\\times\\left[\\begin{array}{c}\n",
    "w_{1}\\\\\n",
    "w_{2}\\\\\n",
    "w_{3}\n",
    "\\end{array}\\right]$\n",
    "\n",
    "**Further**, what if we have two samples instead of one? (Recall Q1.2) We can simply expand the $X$-part of the above computation, where _rows represent samples_ (with an extra subscript).\n",
    "\n",
    "$\\left[\\begin{array}{ccc}\n",
    "x_{1,1} & x_{1,2} & x_{1,3}\\\\\n",
    "x_{2,1} & x_{2,2} & x_{2,3}\n",
    "\\end{array}\\right]\\times\\left[\\begin{array}{c}\n",
    "w_{1}\\\\\n",
    "w_{2}\\\\\n",
    "w_{3}\n",
    "\\end{array}\\right] \\mapsto \\left[\\begin{array}{c}\n",
    "y_{1}\\\\\n",
    "y_{2}\n",
    "\\end{array}\\right]$\n",
    "\n",
    "**Further more**, what if we have two more linear models, i.e. three outputs together for each data sample? We can simply expand the $W$-part of the above computation, where _columns represent individual models_.\n",
    "\n",
    "$\\left[\\begin{array}{ccc}\n",
    "x_{1,1} & x_{1,2} & x_{1,3}\\\\\n",
    "x_{2,1} & x_{2,2} & x_{2,3}\n",
    "\\end{array}\\right]\\times\\left[\\begin{array}{ccc}\n",
    "w_{1,1} & w_{1,2} & w_{1,3}\\\\\n",
    "w_{2,1} & w_{2,2} & w_{2,3}\\\\\n",
    "w_{3,1} & w_{3,2} & w_{3,3}\n",
    "\\end{array}\\right] \\mapsto\\left[\\begin{array}{ccc}\n",
    "y_{1,1} & y_{1,2} & y_{1,3}\\\\\n",
    "y_{2,1} & y_{2,2} & y_{2,3}\n",
    "\\end{array}\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q2__\n",
    "1. Write out the computation for $y_{1,2}$ in the last formulation.\n",
    "2. If we want to compute a further layer, using 3 $Y$-variables as input, to output 2 outputs, say $z_{i,1}, z_{i,2}$ for a sample $i$. Write out the matrix formulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER TO Q2.2**\n",
    "\n",
    "Following the computation of $Y$,\n",
    "$\\left[\\begin{array}{ccc}\n",
    "y_{1,1} & y_{1,2} & y_{1,3}\\\\\n",
    "y_{2,1} & y_{2,2} & y_{2,3}\n",
    "\\end{array}\\right]\\times\\left[\\begin{array}{cc}\n",
    "u_{1,1} & u_{1,2}\\\\\n",
    "u_{2,1} & u_{2,2}\\\\\n",
    "u_{3,1} & u_{3,2}\n",
    "\\end{array}\\right]\\mapsto\\left[\\begin{array}{cc}\n",
    "z_{1,1} & z_{1,2}\\\\\n",
    "z_{2,1} & z_{2,2}\n",
    "\\end{array}\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Non-linear construction\n",
    "\n",
    "The \"multilayer\" models above are illusion! All multi-stage linear models above are equivalent to single layer models.\n",
    "\n",
    "__Q3__\n",
    "\n",
    "Can you show for the above example $X\\mapsto Y \\mapsto Z$, how the two stage models are equivalent to one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER TO Q3**\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Z & =Y\\times U\\\\\n",
    "Y & =X\\times W\\\\\n",
    "Z & =X\\times W\\times U\\\\\n",
    "Z & =X\\times V,\\\\\n",
    "V & :=W\\times U\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Note $V$ is a definition. Using a linear model parameterised by $V$, we achieve the effect of two models in one shot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LAB**\n",
    "\n",
    "1. Verify the above construction -- following the example codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "example_X = torch.rand(2, 3)\n",
    "linear_model_W = nn.Linear(in_features=3, out_features=3, bias=False)\n",
    "linear_model_U = nn.Linear(in_features=3, out_features=2, bias=False)\n",
    "linear_model_V = nn.Linear(in_features=3, out_features=2, bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Verify the computation of linear model following the matrix multiplication\n",
    "\n",
    "- is the output according to expectation? if not, why and how to fix?\n",
    "    - Hint-1: does `pytorch` represent one linear model in a _column_ in the weight matrix? Review the outputs in Q1_Lab.2.\n",
    "    - Hint-2: check `transpose` method in tensors.\n",
    "    - Please note similar introspection is useful for all frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Applying linear model \\n\", linear_model_W(example_X))\n",
    "print(\"Matrix multiplication between X and Weight Matrix \\n\", \n",
    "      torch.mm(example_X, linear_model_W.weight)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "print(\"Matrix multiplication between X and Weight Matrix \\n\", \n",
    "      torch.mm(example_X, linear_model_W.weight.transpose(1,0))\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Let us replace the parameters of V with product of W and U\n",
    "    - Hint: assign `linear_model_V.weight.data` appropriate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "linear_model_V.weight.data = \\\n",
    "    torch.mm(linear_model_U.weight, linear_model_W.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Transformed by W then U\\n\", linear_model_U(linear_model_W(example_X)))\n",
    "print(\"Transformed by V\\n\", linear_model_V(example_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-linear activation\n",
    "\n",
    "- Elementwise transform, e.g. \n",
    "$\\frac{1}{1+\\exp(-x)}$ or $0$ if $x<0$, $x$ otherwise\n",
    "\n",
    "**Q4**\n",
    "\n",
    "1. Draw plots of the above two functions for $x \\in [-3, 3]$\n",
    "    - Hint-1: the first activation is called \"Sigmoid\" and the second one \"ReLU\" (rectified linear). In `torch.nn` module, their are classes \"Sigmoid\" and \"ReLU\", from which you can instantiate the activators. In `torch.nn.functional` module, however, there are corresponding activators. The choice is often upto the developer's style.\n",
    "2. How many parameters learnable in the activators?\n",
    "3. [**LAB**] Verify that after applying an activation, the two step transform in the example above is no longer collapsing into one. \n",
    "4. Construct a model accepting 4 attributes, transform to 2 features, do non-linear activation and then into 3 outputs.\n",
    "    - Hint-2: see the template definition below\n",
    "    - Hint-3: Let's use a `softmax` activation for the final layer.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = torch.arange(-3, 3, 0.01)\n",
    "x_np = x.numpy()\n",
    "# you many want to check \n",
    "# x_np2 = np.arange(-3, 3, 0.01) \n",
    "# is the same as x_np, numpy and torch arrays are easily convertible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ANSWER to (one of many) 1.\n",
    "f1 = nn.Sigmoid()\n",
    "y1 = f1(x)\n",
    "y2 = nn.functional.relu(x)\n",
    "plt.plot(x_np, y1.numpy())\n",
    "plt.plot(x_np, y2.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER to 3\n",
    "# Template definition of an NN model\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.linear_layer1 = nn.Linear(\n",
    "            in_features=4,\n",
    "            out_features=2\n",
    "        )\n",
    "        self.linear_layer2 = nn.Linear(\n",
    "            in_features=2,\n",
    "            out_features=3\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        This is a piece of comments for functions\n",
    "        :param x: x the input data\n",
    "        :type x: torch.FloatTensor\n",
    "        \"\"\"\n",
    "        h = self.linear_layer1(x)\n",
    "        h = nn.functional.relu(h)\n",
    "        h = self.linear_layer2(h)\n",
    "        y = nn.functional.softmax(h, dim=1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given data x, the usage will be:\n",
    "x = torch.randn(10, 4)\n",
    "model = MyModel()\n",
    "results = model(x)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# A Complex NN Example\n",
    "\n",
    "----\n",
    "The model is adopted from CycleGAN, see the project [page](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AVAILABLE_TARGET_STYLES = [\n",
    "    \"apple2orange\", \"orange2apple\", \n",
    "    \"summer2winter_yosemite\", \"winter2summer_yosemite\", \n",
    "    \"horse2zebra\", \"zebra2horse\", \"monet2photo\", \n",
    "    \"style_monet\", \"style_cezanne\", \"style_ukiyoe\", \n",
    "    \"style_vangogh\", \"sat2map\", \"map2sat\", \n",
    "    \"cityscapes_photo2label\", \"cityscapes_label2photo\", \n",
    "    \"facades_photo2label\", \"facades_label2photo\", \"iphone2dslr_flower\"\n",
    "]\n",
    "\n",
    "TARGET_STYLE = AVAILABLE_TARGET_STYLES[10]\n",
    "print(\"TARGET_STYLE: \", TARGET_STYLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download trained style-conversion models\n",
    "import os\n",
    "import urllib.request\n",
    "model_path = \"ref/saved_style_models/\" + TARGET_STYLE + \".pth\"\n",
    "if not os.path.exists(model_path):\n",
    "    urllib.request.urlretrieve(\n",
    "        \"http://efrosgans.eecs.berkeley.edu/cyclegan/pretrained_models/\" + \\\n",
    "        TARGET_STYLE + \".pth\",\n",
    "        model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cganimstyler as cim\n",
    "# build the style model\n",
    "netG = cim.load_generator_from(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below performs the conversion. Each pixel in the target image is the answer of a series of linear models. The model is defined in `cganimstyler/resnet.py`. The pre-trained parameters are saved in `ref/saved_style_models`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = cim.load_image('ref/testimages/Jun.jpeg') # Put your own image here!\n",
    "res = netG(im)\n",
    "npim = cim.tensor2im(im)\n",
    "res_npim = cim.tensor2im(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.imshow(npim)\n",
    "plt.axis('off')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(res_npim)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# TRAINING\n",
    "\n",
    "**Back-propagation** interpreted.\n",
    "\n",
    "We will practice a simple demo of this algorithm on class.\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5**: Train the model defined in Q4.4 to classify Iris Data (provided by scikit-learn, see below)\n",
    "\n",
    "1. Prepare data\n",
    "    - Hint: template is provided below\n",
    "    - Why train-validation split?\n",
    "    - What is the random seed use for?\n",
    "    - **LAB**: try and understand the data preparation steps. Specifically, understanding the definition of the following objects in terms of \"duck typing\", i.e. their implementation and utility. Consult your tutors for any confusion.\n",
    "        - Dataset\n",
    "        - Dataset split\n",
    "            - Hint: you will have two / three subsets\n",
    "        - Shuffling\n",
    "        - Random seeding\n",
    "    - **LAB**: (optional) consider normalising the variables\n",
    "2. Define the objective. (TBC below)\n",
    "3. Calculate the direction of change\n",
    "4. Apply the change. (TBC below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Template data loading procedure: Q5-1.1\n",
    "import random\n",
    "from sklearn.datasets import load_iris\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler # train/valid-subset sampler\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Such an object can be handled by a \"Loader\" object. \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(FlowerDataset, self).__init__()\n",
    "        self._data = load_iris()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._data.data)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        So you can use dataset[i]\n",
    "        \"\"\"\n",
    "        sample = (torch.FloatTensor(self._data.data[i]), \n",
    "                  int(self._data.target[i]))\n",
    "        return sample\n",
    "\n",
    "dataset = FlowerDataset()\n",
    "VALID_RATIO = 0.2\n",
    "valid_num = int(len(dataset)*VALID_RATIO)\n",
    "\n",
    "print(\"Use {} samples for training, {} for validation\".format(\n",
    "    len(dataset)-valid_num, valid_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "indices = list(range(len(dataset)))\n",
    "random.shuffle(indices)\n",
    "train_indices = indices[valid_num:] # check Python indexing\n",
    "valid_indices = indices[:valid_num]\n",
    "print(train_indices, valid_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(valid_indices)\n",
    "train_loader = DataLoader(dataset=dataset, \n",
    "                          sampler=train_sampler, \n",
    "                          batch_size=32)\n",
    "valid_loader = DataLoader(dataset=dataset,\n",
    "                          sampler=valid_sampler,\n",
    "                          batch_size=valid_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the way a DataLoader is used, we break at the first\n",
    "# round to take only one batch of samples.\n",
    "for x, y in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5.2** (cont.)\n",
    "\n",
    "We treate the output of the model as __predicted__ probability of each sample belongs to each class. The discrepancy between the prediction and the ground-truth is _the target value_ to minimise. Before proceeding, let's review the model and make a slight modification\n",
    "\n",
    "- Why we use LOG-softmax + NLL-Loss, instead of using softmax (without log)? [hint](https://pytorch.org/docs/stable/nn.html?highlight=nll%20loss#torch.nn.NLLLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel2, self).__init__()\n",
    "        self.linear_layer1 = nn.Linear(\n",
    "            in_features=4,\n",
    "            out_features=5\n",
    "        )\n",
    "        self.linear_layer2 = nn.Linear(\n",
    "            in_features=5,\n",
    "            out_features=3\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        This is a piece of comments for functions\n",
    "        :param x: x the input data\n",
    "        :type x: torch.FloatTensor\n",
    "        \"\"\"\n",
    "        h = self.linear_layer1(x)\n",
    "        h = nn.functional.relu(h)\n",
    "        h = self.linear_layer2(h)\n",
    "        y = nn.functional.log_softmax(h, dim=1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(x)\n",
    "loss = nn.functional.nll_loss(pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5.3** (cont.)\n",
    "\n",
    "To compute the direction along which to adjust the parameters of the model, now we can simply let `loss` backprop:\n",
    "\n",
    "**LAB**: adjust one or several parameters of the parameter and check the effect on the prediction and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You cannot repeat this OP to overwrite previously computed gradients\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5.4** (cont.)\n",
    "\n",
    "We use an optimiser object to handle the update of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "optimiser = Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser.zero_grad() # reset all computed gradients\n",
    "pred = model(x)\n",
    "loss = nn.functional.nll_loss(pred, y)\n",
    "loss.backward()\n",
    "optimiser.step() # Apply the change\n",
    "print(\"Loss Before {:.6f}\".format(loss))\n",
    "\n",
    "optimiser.zero_grad()\n",
    "pred = model(x)\n",
    "loss = nn.functional.nll_loss(pred, y)\n",
    "loss.backward()\n",
    "optimiser.step() # Apply the change\n",
    "print(\"Loss After {:.6f}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- what happens if executing the cell above for several times?\n",
    "- explain your finding\n",
    "- there is a key element missing if one wants to call repeated iteration of the above cell as \"training\" -- what's the missing piece?\n",
    "- **LAB**: Write the _training_ algorithm\n",
    "- **LAB**: evaluate the model performance on the validation data\n",
    "- why do we need the validation set (or, why don't just call them test set)\n",
    "    - choices such as number of internal neurons can be selected against the model performance on this set\n",
    "    - make the internal nodes adjustable\n",
    "        - Hint: check the definition on `Model2` above\n",
    "    - perform multiple experiments on the train/test split \n",
    "        - Hint: recall the setting of random seed above\n",
    "        \n",
    "- the performance is close to [cross-ref](https://www.kaggle.com/azzion/iris-data-set-classification-using-neural-network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ANSWER AND SOLUTION TO LAB TASKS\n",
    "# let's reset\n",
    "model = MyModel2() \n",
    "optimiser = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "TRAIN_ITERS = 1000\n",
    "EVALUATE_EVERY_N_STEPS = 100\n",
    "total_steps = 0\n",
    "for epoch in range(TRAIN_ITERS):\n",
    "    for x, y in train_loader:\n",
    "        optimiser.zero_grad() # reset all computed gradients\n",
    "        pred = model(x)\n",
    "        loss = nn.functional.nll_loss(pred, y)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        total_steps += 1\n",
    "        if total_steps % EVALUATE_EVERY_N_STEPS == 0:\n",
    "            # compute ACCURACY on VALIDATION SET\n",
    "            total_valid = 0\n",
    "            correct_valid = 0\n",
    "            for x_, y_ in valid_loader:\n",
    "                pred_ = model(x_)\n",
    "                correct_valid += (torch.argmax(pred_, dim=1)==y_).sum()\n",
    "                # ==: element by element comparison\n",
    "                # if == holds, max-in-pred EQUALS TO target class, correct, count 1\n",
    "                # if not, count 0\n",
    "                # at last compare how many we have counted with total validation samples\n",
    "                total_valid += len(y_)\n",
    "            print(\"Training epoch {} (total iter {}),\" \n",
    "                  \"loss {:.6f}, accuracy {:.2f}\".format(\n",
    "                      epoch, total_steps, loss, \n",
    "                      float(correct_valid)/total_valid\n",
    "                  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "nbpresent": {
   "slides": {
    "82f8aeb7-b72b-4703-86c3-c359cedd7b08": {
     "id": "82f8aeb7-b72b-4703-86c3-c359cedd7b08",
     "prev": "e77aad2d-6a89-435d-a3ab-85aecc2b7f04",
     "regions": {
      "8008b8a0-764e-49cc-aea3-4dce4237d204": {
       "attrs": {
        "height": 0.8,
        "width": 0.45,
        "x": 0.5,
        "y": 0.1
       },
       "content": {
        "cell": "f586814d-9bdb-473a-8b62-88f33564f6a3",
        "part": "whole"
       },
       "id": "8008b8a0-764e-49cc-aea3-4dce4237d204"
      },
      "91c774b1-f0c7-415f-95da-0c1d87bf8939": {
       "attrs": {
        "height": 0.8,
        "width": 0.45,
        "x": 0.05,
        "y": 0.1
       },
       "content": {
        "cell": "02b75f38-6bee-40fd-bb82-5854b58fb7e3",
        "part": "whole"
       },
       "id": "91c774b1-f0c7-415f-95da-0c1d87bf8939"
      }
     }
    },
    "e77aad2d-6a89-435d-a3ab-85aecc2b7f04": {
     "id": "e77aad2d-6a89-435d-a3ab-85aecc2b7f04",
     "layout": "manual",
     "prev": null,
     "regions": {
      "10969ddc-8694-42e7-a496-ac26c893f412": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "e74573cd-fc54-4ba3-8586-8101ccdc1e80",
        "part": "source"
       },
       "id": "10969ddc-8694-42e7-a496-ac26c893f412"
      }
     },
     "theme": null
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
