{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "Problem Setting\n",
    "\n",
    "<img src=\"ref/rl-problem.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The data / prediction sequence is like:\n",
    "$$\n",
    "S_0, A_0, r_1, S_1, A_1, r_2, S_2, \\dots\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "NB-1: the subscript stands for the time-step, not to be confused with symbols distinguishing different individual states/actions. E.g. you need to make yourself comfortable with notions like $s_5 = s^3$ or $a_5 = a^{MOVE-RIGHT}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "NB-2: sometimes, a different denotation is taken for the time step of reward, so instead of considering $s_0, a_0, !, r_1, s_1, $, we adopt the denotation system as  $s_0, a_0, r_0, !, s_1$ -- I explicitly use \"!\" for time-step-tick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RL Essential: Exploration\n",
    "\n",
    "A criticism:\n",
    "> Deep RL is popular because it's the only area in ML where it's socially acceptable to train on the test set.\n",
    "-- [A tweet](https://twitter.com/jacobandreas/status/924356906344267776)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is because in RL the agent is not allowed to access data before it's being tested!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Exploration scheme:\n",
    "<img src=\"ref/explore.png\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Examples of RL Environments\n",
    "Let have a look at typical tasks. Here is a [list of environment](https://gym.openai.com/envs/#classic_control) provided by OpenAI Gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "state  = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00115395  0.94757329 -0.11689678  0.46399501  0.00134391  0.02647887\n",
      "  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A typical control flow\n",
    "```python\n",
    "state = env.reset()\n",
    "while not done:\n",
    "    # make t-step decision\n",
    "    act = policy(state) # !MARKOVIAN ASSUM. -- MDP\n",
    "    # POMDP\n",
    "\n",
    "    # step is the way to commit an action\n",
    "    new_state, reward, done, _ = env.step(act) \n",
    "    \n",
    "    # adjust policy on-fly or this can be done after an episode is done\n",
    "    \n",
    "    # loop over\n",
    "    state = new_state\n",
    "```\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "env = gym.make('LunarLander-v2')\n",
    "state = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    # optionally, we can see how everything goes\n",
    "    env.render()\n",
    "    act = random.randint(0, 3) #0 # dummy act\n",
    "    new_state, reward, done, _ = env.step(act) \n",
    "    state = new_state\n",
    "    time.sleep(0.02)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Observations and state\n",
    "- POMDP\n",
    "- Multimedia observations\n",
    "\n",
    "For some game, you get the \"internal\" states to work with. On the otherhand, your agent must rely on it own to interpret some observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Main Families of RL Algorithms\n",
    "\n",
    "## Decision Making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Value Iteration\n",
    "    - Q-learning (DQN)\n",
    "    \n",
    "    Action = $\\arg\\max_a {Q(\\rm{state}, a)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Policy Iteration\n",
    "    - Policy Gradient \n",
    "    \n",
    "    Action ~ $\\pi_\\theta(\\cdot|\\rm{state})$\n",
    "    \n",
    "    $\\theta \\leftarrow (1-\\alpha)\\theta + \\alpha \\Delta\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exploration in the space\n",
    "\n",
    "- MonteCarlo Methods\n",
    "    - To account for variables difficult to model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Temporal Difference Learning\n",
    "    - \"Look into the future\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\"O\": evaluation of the CURRENT version of value estimation\n",
    "<img src=\"ref/td.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Planning\n",
    "    - MonteCarlo Tree Search (AlphaGo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Q-Learning (TD)\n",
    "\n",
    "## A Trivial \"learing Q\" and improve scheme\n",
    "\n",
    "Example from [Sutton and Barto, 2018, \"Reinforcement Learning, An Introduction\" 2nd ed.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"ref/simple-grid-problem.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Starting from (the table shows not $Q$, but $V$), while Q and V are convertible -- at least in this certain, simple world environment.\n",
    "<img src=\"ref/q-step000.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The simple policy is to RANDOMLY take 4 actions at EACH state -- After 1 step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"ref/q-step001.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"ref/q-step002.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"ref/q-step003.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"ref/q-step003.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"ref/q-step010.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"ref/q-step999.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "| Q        | $\\pi$           |\n",
    "|:-------------:|:-------------:| \n",
    "| <img src=\"ref/q-step999.png\" width=\"100%\">|<img src=\"ref/q-policy-opt.png\" width=\"100%\">| \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "| Q        | $\\pi$           |\n",
    "|:-------------:|:-------------:| \n",
    "| <img src=\"ref/q-step000.png\" width=\"100%\">|<img src=\"ref/q-policy-000.png\" width=\"100%\">| "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "| Q        | $\\pi$           |\n",
    "|:-------------:|:-------------:| \n",
    "| <img src=\"ref/q-step001.png\" width=\"100%\">|<img src=\"ref/q-policy-001.png\" width=\"100%\">| "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "| Q        | $\\pi$           |\n",
    "|:-------------:|:-------------:| \n",
    "| <img src=\"ref/q-step002.png\" width=\"100%\">|<img src=\"ref/q-policy-002.png\" width=\"100%\">| "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab\n",
    "\n",
    "Play with a grid-game \"[FrozenLake](https://gym.openai.com/envs/FrozenLake-v0/)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test shallow water first.\n",
    "from gym.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "    max_episode_steps=2000,\n",
    "    reward_threshold=0.78, # optimum = .8196\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Demo solution -- please try your own method before playing with this piece of code.\n",
    "\n",
    "slipery = True\n",
    "env = gym.make('FrozenLake-v0') if slipery \\\n",
    "    else gym.make('FrozenLakeNotSlippery-v0')\n",
    "\n",
    "######### HYPER PARAMETERS ########\n",
    "lr = 0.1\n",
    "gamma = 0.9 # How far into the future we are looking at\n",
    "Q = torch.zeros(env.observation_space.n, env.action_space.n)\n",
    "rs_longterm = 0\n",
    "report_ever_n_episodes = 5000\n",
    "n_episodes = 150000\n",
    "\n",
    "# ! Exploration vs Exploitation \n",
    "randomness = 1.0\n",
    "epsilon = 0.01\n",
    "explore_steps = n_episodes * 2\n",
    "d_random = (randomness-epsilon)/explore_steps\n",
    "\n",
    "######### LEARNING ########\n",
    "for ep in range(n_episodes):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    rs = 0\n",
    "    while not done:\n",
    "        if random.random()>randomness or ep > n_episodes - report_ever_n_episodes:\n",
    "            _, a = Q[state].max(dim=0)\n",
    "            a = a.item()\n",
    "        else:\n",
    "            a = random.randint(0, env.action_space.n-1)\n",
    "            \n",
    "        new_state, reward, done, _ = env.step(a)\n",
    "        if ep == n_episodes-1:\n",
    "            print(Q[state])\n",
    "            print(a, Q[state].max(dim=0))\n",
    "            env.render()\n",
    "\n",
    "\n",
    "        rs += reward\n",
    "        Q[state, a] +=  lr*(reward + gamma*Q[new_state].max() - Q[state, a])\n",
    "        state = new_state\n",
    "        randomness = max(epsilon, randomness - d_random)\n",
    "    if (ep+1) % report_ever_n_episodes == 0:\n",
    "        print(ep, rs_longterm/report_ever_n_episodes)\n",
    "        print(Q.max(dim=1)[1])\n",
    "        rs_longterm = 0\n",
    "    rs_longterm += rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LET'S CHECK Q!\n",
    "Q"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
