{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def print_lists(LL, names):\n",
    "    for l_, n_ in zip(LL, names):\n",
    "        l = n_ + ':'\n",
    "        for e_ in l_:\n",
    "            l += \"{:5.2f} \".format(e_)\n",
    "        print (l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ensemble Methods\n",
    "\n",
    "\"Ensemble\" is an umbralla term\n",
    "- progressively build up model capacity w.r.t. fitness to data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Ensemble: combination of multiple predictors.\n",
    "- General idea is to construct a set of base predictors (hypotheses) from training data, then to predict unseen data we combine the predictions of the base predictors by voting. \n",
    "- Ensemble can be considered as a family of “meta-learning” methods. It is even possible to combine different kinds of data models, such as NN + Decision Trees + … （though a bit unusual)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Ensemble methods tend to perform better than individual classifiers \n",
    "- Ensemble tends to be more reliable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- a bit blurred boundary between the \"Algorithm\" and \"Hypothesis space\" in our familiar map of learning framework\n",
    "<img src=\"ref/learning.png\" width=\"400\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Standard Hypothesis space\n",
    "    - Each $h \\in \\mathcal H$ is a map $\\mathcal X \\mapsto \\mathcal Y$ (recall the math notions)\n",
    "    - Learning algorithm consider one $h$ per step\n",
    "        ```\n",
    "        while do searching\n",
    "            compare current h(x_train) with y_train\n",
    "            update h\n",
    "        ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Ensemble: \"weak\" hypothesis space $\\mathcal H^W$ PLUS hypothesis space $\\mathcal H$\n",
    "    - search (and final model) is in $\\mathcal H$\n",
    "    - each $h \\mathcal H$ is a voting scheme by a committee $\\{h^w_1, h^w_2, \\dots \\} \\subset \\mathcal H^W$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Case study-1\n",
    "Suppose we have an ensemble of $m$ (odd) binary classifiers, each of accuracy $\\gamma$. The ensemble scheme is a majority vote. Such as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"ref/3vote.png\" width=\"800\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Q1__\n",
    "Compute the ensemble accuracy in the first example given $m$ and $\\gamma$. \n",
    "- Hint: you can avoid math derivation by adopting empirical evaluation. (example provided)\n",
    "- Do the curve for more cases of $\\gamma=0.51$ and $\\gamma=0.9$\n",
    "- Does trails $N$ have any effect on the result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def comp_committee_accu(m, gm, N=1000, randseed=42):\n",
    "    rng = np.random.RandomState(randseed)\n",
    "    is_correct = (rng.rand(m, N)<gm).astype(np.float)\n",
    "    # simulate individual classifier in individual problems\n",
    "    # if you are not comfortable with the idea of N trails simultaneously\n",
    "    # imagine N=1 for now.\n",
    "    \n",
    "    votes_correct = is_correct.sum(axis=0) # how many committee members are correct \n",
    "    corr = (votes_correct > m/2.0).astype(np.float)\n",
    "    return corr.sum() / float(N)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "members = np.arange(1, 200, 2)\n",
    "accu = [0.6, 0.7, 0.51, 0.9]\n",
    "accu_curv_1 = [] \n",
    "accu_curv_2 = [] \n",
    "for mn in members:\n",
    "    accu_curv_1.append(comp_committee_accu(mn, accu[0]))\n",
    "    accu_curv_2.append(comp_committee_accu(mn, accu[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(members, accu_curv_1, 'b')\n",
    "plt.plot(members, accu_curv_2, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A (really) Quick Glance of Emsemble Methods\n",
    "\n",
    "```\n",
    "while training:\n",
    "    given current committee_h:{h1, h2, ..., h_k}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Bagging: $h_{k+1} \\in \\mathcal H^W \\leftarrow \\mathcal A($  random subset of training data $)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Boosting: $h_{k+1} \\in \\mathcal H^W \\leftarrow \\mathcal A($  random subset of training data __with weights__ $)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Random forest: $h_{k+1} \\in \\mathcal H^W \\leftarrow \\mathcal A($  random subset of training data __with weights__ and __selected attributes__ $)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# AdaBoost\n",
    "- for $t$ in $\\{1, 2, \\dots, T\\}$\n",
    "    - $h_t \\leftarrow \\arg\\min_{h \\in \\mathcal H^W} \\big[\\epsilon_t(D^t, X)\\big]$\n",
    "        - $\\epsilon_t(D_t, X)$: weighted error: $\\sum_i D^t_i X_i$\n",
    "    - weighting $h_t$: $\\alpha_t \\leftarrow \\frac{1}{2}\\log \\frac{1-\\epsilon_t^*}{\\epsilon_t^*}$\n",
    "    - update sample weights: $D'^t_i \\leftarrow D^t_i \\exp(\\pm \\alpha_t)$, \n",
    "        - then normalise to $D^t_i$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Two Kinds of Weights\n",
    "- Weights of Samples: each round we \"adjust our focus\" on the hard samples.\n",
    "- Weights of Weak Classifiers: utility of the classifier\n",
    "\n",
    "### A hand-on example\n",
    "- X: data\n",
    "- Y: target values\n",
    "- D: initial data weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X = np.arange(0.1, 1.01, 0.1)\n",
    "Y = np.asanyarray([1, 1, 1, -1, -1, -1, -1, 1, 1, 1])\n",
    "D = np.ones_like(Y).astype(float) / X.size\n",
    "print_lists([X, Y, D], ['X', 'Y', 'D'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Alternative ways of utilising the data weights\n",
    "1. directly weigting\n",
    "```\n",
    "Error = D[i] * Loss(pred[i], Y[i])\n",
    "```\n",
    "2. Monte Carlo scheme (see below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1-1 Generating Random Samples\n",
    "rng = np.random.RandomState(0)\n",
    "ind = rng.choice(10, size=(10, ), p=D)\n",
    "ind.sort()\n",
    "X_ = X[ind]\n",
    "Y_ = Y[ind]\n",
    "print_lists([X_, Y_], ['X_', 'Y_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's verify the scheme -- in the limit of large sample size\n",
    "ind = rng.choice(10, size=(100000, ), p=D)\n",
    "plt.hist(ind, rwidth=0.9, bins=np.arange(11)-0.5, align='mid')\n",
    "plt.xlim([-0.7, 9.7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's 'train' a week predictor -- it is no more than a majority vote!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1-2 \"Training\" a week predictor\n",
    "predictor_1 = lambda x: 1 if x >= 0.75 else -1\n",
    "output_trn = np.asanyarray(list(map(predictor_1, X_)))\n",
    "output_1 = np.asanyarray(list(map(predictor_1, X)))\n",
    "is_correct = np.asanyarray(output_1 == Y).astype(np.float)\n",
    "print_lists([X_, Y_, output_trn, \n",
    "             X, Y, output_1, is_correct], [ 'TrnX', 'TrnY', 'TrnP', '   X', '   Y', 'Pred', 'Corr',])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Assess the outcome:\n",
    "- $\\epsilon_1(D^1, X) := \\sum_i D^1_i Loss(Pred_i, Y_i)$ -- weighted error\n",
    "- $\\alpha_1 := 0.5 \\log \\frac{1-\\epsilon_1}{\\epsilon_1}$ -- smaller the error, better (and more important) this classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1-3 Predictor Weight\n",
    "weighted_errors = (1.0 - is_correct) * D\n",
    "eps_1 = (weighted_errors).sum()\n",
    "alpha_1 = 0.5 * np.log( (1-eps_1) / eps_1 )\n",
    "print_lists([X, D, weighted_errors], ['   X', 'Wght', 'WErr'])\n",
    "\n",
    "print(\"Alpha = {:.3f}\".format(alpha_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Adjust weights according to this round outcome:\n",
    "- $D^2_i \\leftarrow D^1_i \\exp(\\alpha_1)$: if \"difficult\" (wrongly classified)\n",
    "- $\\ \\ \\ \\ \\  \\leftarrow D^1_i \\exp(-\\alpha_1)$: if \"easy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1-4 New sample weights\n",
    "sample_signs_1 = (0.5 - is_correct) * 2.0\n",
    "D = D * np.exp(sample_signs_1 * alpha_1)\n",
    "D /= D.sum()\n",
    "print_lists([X, D], ['   X', 'NewW'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's verify the scheme -- in the limit of large sample size\n",
    "ind = rng.choice(10, size=(100000, ), p=D)\n",
    "plt.hist(ind, rwidth=0.9, bins=np.arange(11)-0.5, align='mid')\n",
    "plt.xlim([-0.7, 10.7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ROUND-2 ...\n",
    "\n",
    "Now let's start over to add a new weak classifier. Note the data have uneven weights now. Draw training samples first, hopefully, we can have those needing improvement over represented this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Step 2-1 Re draw-samples ...\n",
    "ind = rng.choice(10, size=(10, ), p=D)\n",
    "ind.sort()\n",
    "X_ = X[ind]\n",
    "Y_ = Y[ind]\n",
    "\n",
    "print_lists([X_, Y_], ['X_', 'Y_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Step 2-2 \"Training\" the 2nd week predictor\n",
    "predictor_2 = lambda x: 1 if x <=0.35 else -1\n",
    "output_2 = np.asanyarray(list(map(predictor_2, X)))\n",
    "is_correct = np.asanyarray(output_2 == Y).astype(np.float)\n",
    "print_lists([X, Y, output_2, is_correct], ['   X', '   Y', 'Pred', 'Corr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Assess the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Step 2-3 Predictor Weight\n",
    "weighted_errors = (1.0 - is_correct) * D\n",
    "eps_2 = (weighted_errors).sum()\n",
    "alpha_2 = 0.5 * np.log( (1-eps_2) / eps_2 )\n",
    "print_lists([X, D, weighted_errors], ['   X', 'Wght', 'WErr'])\n",
    "\n",
    "print (\"Alpha = {:.3f}\".format(alpha_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Update the sample weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Step 2-4 New sample weights\n",
    "sample_signs_2 = (0.5 - is_correct) * 2.0\n",
    "D = D * np.exp(sample_signs_2 * alpha_2)\n",
    "D /= D.sum()\n",
    "print_lists([X, D], ['   X', 'NewW'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let us test the power of ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def predict_ensemble(X, ens):\n",
    "    print(\"       X:\", end=\"\")\n",
    "    for x_ in X:\n",
    "        print ('{:5.2f}'.format(x_), end=\"\")\n",
    "    print()\n",
    "    \n",
    "    ens_p = np.zeros_like(X)\n",
    "    for p, a in ens:\n",
    "        this_pred_ = np.asanyarray(list(map(p, X)))\n",
    "        \n",
    "        ens_p += this_pred_ * a\n",
    "        \n",
    "        print (\"A={:6.2f}:\".format(a), end=\"\")\n",
    "        for pd_ in this_pred_:\n",
    "            print (\"{:5.2f}\".format(pd_*a), end=\"\")\n",
    "        print()\n",
    "        \n",
    "    print (\"Final(R):\".format(a), end=\"\")\n",
    "    for pd_ in ens_p:\n",
    "        print (\"{:5.2f}\".format(pd_), end=\"\")\n",
    "    print()\n",
    "    print (\"Final(C):\".format(a), end=\"\")\n",
    "    for pd_ in ens_p:\n",
    "        print (\"{:5.2f}\".format(np.sign(pd_)), end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ensemble = [(predictor_1, alpha_1), \n",
    "            (predictor_2, alpha_2)]\n",
    "predict_ensemble(X, ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ROUND-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Step 3-1 Re draw-samples ...\n",
    "ind = rng.choice(10, size=(10, ), p=D)\n",
    "ind.sort()\n",
    "X_ = X[ind]\n",
    "Y_ = Y[ind]\n",
    "\n",
    "print_lists([X_, Y_], ['X_', 'Y_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Step 3-2 \"Training\" the 3rd week predictor\n",
    "predictor_3 = lambda x: 1 if x>=0.75 else -1\n",
    "output_3 = np.asanyarray(list(map(predictor_3, X)))\n",
    "is_correct = np.asanyarray(output_3 == Y).astype(np.float)\n",
    "print_lists([X, Y, output_3, is_correct], ['   X', '   Y', 'Pred', 'Corr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Step 3-3 Predictor Weight\n",
    "weighted_errors = (1.0 - is_correct) * D\n",
    "eps_3 = (weighted_errors).sum()\n",
    "alpha_3 = 0.5 * np.log( (1-eps_3) / eps_3 )\n",
    "print_lists([X, D, weighted_errors], ['   X', 'Wght', 'WErr'])\n",
    "\n",
    "print (\"Alpha = {:.3f}\".format(alpha_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Step 3-4 New sample weights\n",
    "sample_signs_3 = (0.5 - is_correct) * 2.0\n",
    "D = D * np.exp(sample_signs_3 * alpha_3)\n",
    "D /= D.sum()\n",
    "print_lists([X, D], ['   X', 'NewW'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ensemble = [(predictor_1, alpha_1), \n",
    "            (predictor_2, alpha_2),\n",
    "            (predictor_3, alpha_3)]\n",
    "predict_ensemble(X, ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ROUND-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Step 4-1 Re draw-samples ...\n",
    "ind = rng.choice(10, size=(10, ), p=D)\n",
    "ind.sort()\n",
    "X_ = X[ind]\n",
    "Y_ = Y[ind]\n",
    "\n",
    "print_lists([X_, Y_], ['X_', 'Y_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Step 4-2 \"Training\" the 4st week predictor\n",
    "predictor_4 = lambda x: 1 if x < 999 else -1\n",
    "output_4 = np.asanyarray(list(map(predictor_4, X)))\n",
    "is_correct = np.asanyarray(output_4 == Y).astype(np.float)\n",
    "print_lists([X, Y, output_4, is_correct], ['   X', '   Y', 'Pred', 'Corr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Step 4-3 Predictor Weight\n",
    "weighted_errors = (1.0 - is_correct) * D\n",
    "eps_4 = (weighted_errors).sum()\n",
    "alpha_4 = 0.5 * np.log( (1-eps_4) / eps_4 )\n",
    "print_lists([X, D, weighted_errors], ['   X', 'Wght', 'WErr'])\n",
    "\n",
    "print (\"Alpha = {:.3f}\".format(alpha_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Step 4-4 New sample weights\n",
    "sample_signs_4 = (0.5 - is_correct) * 2.0\n",
    "D = D * np.exp(sample_signs_4 * alpha_4)\n",
    "D /= D.sum()\n",
    "print_lists([X, D], ['   X', 'NewW'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ensemble = [(predictor_1, alpha_1), \n",
    "            (predictor_2, alpha_2),\n",
    "            (predictor_3, alpha_3),\n",
    "            (predictor_4, alpha_4),]\n",
    "predict_ensemble(X, ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LAB\n",
    "\n",
    "We consider building simple classifers on 2D points to approximate interesting data distributions. Under \"Code Adaboost\". Some preparation / helper codes are provided. The answer is provided (commented out block), but it is recommended to try yourself before check with the provided code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Generate data\n",
    "SAMPLE_NUM_PER_CLASS = 200\n",
    "rng = np.random.RandomState(0)\n",
    "X0  = rng.randn(SAMPLE_NUM_PER_CLASS, 2) * 0.15\n",
    "X1_a  = rng.rand(SAMPLE_NUM_PER_CLASS) * 2 * np.pi\n",
    "X1_r  = rng.rand(SAMPLE_NUM_PER_CLASS) * 0.2 + 0.4\n",
    "X1 = np.asarray([np.cos(X1_a)*X1_r, np.sin(X1_a)*X1_r]).T\n",
    "X = np.concatenate((X0, X1), axis=0)\n",
    "y = np.concatenate((np.zeros(SAMPLE_NUM_PER_CLASS), \n",
    "                    np.ones(SAMPLE_NUM_PER_CLASS)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1], c=y, s=32, cmap='summer', edgecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# function / return value / list / python-construction of list / product\n",
    "# Define weak hypothesis\n",
    "\n",
    "class FlatCut:\n",
    "    \"\"\"\n",
    "    2D flat lines to cut a squre\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.mode = 'Undetermined'\n",
    "        self.th = None\n",
    "        \n",
    "    def predict(self, data):\n",
    "        if self.mode == 'horizontal_gt':\n",
    "            pred = data[:, 0] >= self.th\n",
    "        elif self.mode == 'horizontal_lt':\n",
    "            pred = data[:, 0] < self.th\n",
    "        elif self.mode == 'vertical_gt':\n",
    "            pred = data[:, 1] >= self.th\n",
    "        elif self.mode == 'vertical_lt':\n",
    "            pred = data[:, 1] < self.th\n",
    "        else:\n",
    "            assert False, \"Unknown mode \"\n",
    "            \n",
    "        return pred.astype(np.float) \n",
    "            \n",
    "    def fit(self, data, targets):\n",
    "        self.target_values = np.unique(targets)\n",
    "        assert self.target_values.size <= 2, \"Deal with 2-class problem only\"\n",
    "        \n",
    "        if self.target_values.size == 1:\n",
    "            self.mode = 'horizontal_gt'\n",
    "            self.th = - inf # dummy prediction\n",
    "        \n",
    "        # \"moving knife\" cutting at some x-pos\n",
    "        xmin, xmax = data[:, 0].min(), data[:, 0].max()\n",
    "        ymin, ymax = data[:, 1].min(), data[:, 1].max()\n",
    "        best_th = None\n",
    "        best_mode = None\n",
    "        best_accuracy = 0\n",
    "        for self.mode in ['horizontal_gt', 'horizontal_lt']:\n",
    "            for self.th in np.linspace(xmin, xmax, 100):\n",
    "                accu = np.count_nonzero(self.predict(data)==targets) / float(targets.size)\n",
    "                if accu > best_accuracy:\n",
    "                    best_mode = self.mode\n",
    "                    best_th = self.th\n",
    "                    best_accuracy = accu\n",
    "                    \n",
    "        for self.mode in ['vertical_gt', 'vertical_lt']:\n",
    "            for self.th in np.linspace(ymin, ymax, 100):\n",
    "                accu = np.count_nonzero(self.predict(data)==targets) / float(targets.size)\n",
    "                if accu > best_accuracy:\n",
    "                    best_mode = self.mode\n",
    "                    best_th = self.th\n",
    "                    best_accuracy = accu\n",
    "                \n",
    "                \n",
    "        self.th = best_th\n",
    "        self.mode = best_mode\n",
    "        print (self.mode, self.th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "wc = FlatCut()\n",
    "wc.fit(X, y)\n",
    "p = wc.predict(X)\n",
    "plt.scatter(X[:,0], X[:,1], c=p, s=32, cmap='summer', edgecolor='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def ensemble_pred(X, ensemble):\n",
    "    N = X.shape[0]\n",
    "    A0 = np.zeros(N)\n",
    "    A1 = np.zeros(N)\n",
    "    \n",
    "    for p, a in ensemble:\n",
    "        pred = p.predict(X)\n",
    "        A0[pred==0] += a\n",
    "        A1[pred==1] += a\n",
    "        \n",
    "    return (A1>A0).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# AdaBoost\n",
    "rng = np.random.RandomState(0)\n",
    "N = X.shape[0]\n",
    "subsample_size = N\n",
    "T = 50\n",
    "D = np.ones(N)/X.shape[0] # initial distribution\n",
    "ind = rng.choice(X.shape[0], size=(subsample_size,), p=D)\n",
    "X_ = X[ind]\n",
    "y_ = y[ind]\n",
    "ensemble = []\n",
    "for t in range(T):\n",
    "    \n",
    "    weak_pred_t = FlatCut()\n",
    "    weak_pred_t.fit(X_, y_) # RANSAC SimpleModel: Y -> X' ~ X \n",
    "\n",
    "    # ANSWER #\n",
    "#     pred = weak_pred_t.predict(X)\n",
    "#     errors = (pred != y).astype(np.float)\n",
    "#     errors_w = errors * D\n",
    "#     eps_t = max(errors_w.sum(), 1e-6)           # to avoid singular division\n",
    "    \n",
    "#     #  compute hypothesis(predictor) weight\n",
    "#     alpha = 0.5 * np.log((1-eps_t) / eps_t)\n",
    "    \n",
    "#     D *= np.exp( (errors - 0.5) * 2.0 * alpha ) # adjust sample weights \n",
    "#     D /= D.sum()                                # normalise to 1\n",
    "    \n",
    "#     # add to ensemble\n",
    "#     ensemble.append((weak_pred_t, alpha))\n",
    "    \n",
    "\n",
    "    # visualise\n",
    "    plt.figure(1, (12, 4))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.scatter(X_[:,0], X_[:,1], c=weak_pred_t.predict(X_), cmap='summer', s=64, edgecolor='k')\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.scatter(X[:,0], X[:,1], c=ensemble_pred(X, ensemble), cmap='summer', s=64, vmin=0, vmax=1, edgecolor='k')\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.scatter(X[:,0], X[:,1], c=D, cmap='hot', s=64, vmin=0, vmax=D.max(), edgecolor='k')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    ind = rng.choice(X.shape[0], size=(subsample_size,), p=D)\n",
    "    \n",
    "    X_ = X[ind]\n",
    "    y_ = y[ind]\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
